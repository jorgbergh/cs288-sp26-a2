"""
Neural network utilities for Transformer implementation.
Contains basic building blocks: softmax, cross-entropy, gradient clipping, token accuracy, perplexity.
"""
import torch
from torch import Tensor


def softmax(x: Tensor, dim: int = -1) -> Tensor:
    """
    Compute softmax along the specified dimension.
    
    Args:
        x: Input tensor of any shape
        dim: Dimension along which to compute softmax (default: -1)
    
    Returns:
        Tensor of same shape as input with softmax applied along dim
    """
    # TODO: Implement numerically stable softmax. You can re-use the same one 
    # used in part 2. But for this problem, you need to implement a numerically stable version to pass harder tests.
    # Subtract max for numerical stability before exponentiating
    max_x, _ = x.max(dim=dim, keepdim=True)
    shifted = x - max_x
    exp_x = torch.exp(shifted)
    sum_exp = exp_x.sum(dim=dim, keepdim=True)
    return exp_x / sum_exp


def cross_entropy(logits: Tensor, targets: Tensor) -> Tensor:
    """
    Compute cross-entropy loss.
    
    Args:
        logits: Unnormalized log probabilities of shape (N, C) where N is batch size
                and C is number of classes
        targets: Ground truth class indices of shape (N,)
    
    Returns:
        Scalar tensor containing the mean cross-entropy loss
    """
    # TODO: Implement cross-entropy loss
    # Cross-entropy = -log(softmax(logits)[target])
    # Numerically stable: use log_softmax = log(softmax) = logits - log(sum(exp(logits)))
    # Then take negative log likelihood: -log_softmax[target]
    
    # Compute log_softmax (numerically stable)
    max_logits, _ = logits.max(dim=-1, keepdim=True)
    shifted = logits - max_logits
    exp_shifted = torch.exp(shifted)
    sum_exp = exp_shifted.sum(dim=-1, keepdim=True)
    log_softmax = shifted - torch.log(sum_exp)
    
    # Select log probability for target class
    nll = -log_softmax.gather(1, targets.unsqueeze(1)).squeeze(1)
    
    # Return mean loss
    return nll.mean()


def gradient_clipping(parameters, max_norm: float) -> Tensor:
    """
    Clip gradients of parameters by global norm.
    
    Args:
        parameters: Iterable of parameters with gradients
        max_norm: Maximum allowed gradient norm
    
    Returns:
        The total norm of the gradients before clipping
    """
    # TODO: Implement gradient clipping
    # Compute global L2 norm of all gradients
    # Collect all gradient norms squared
    grads = [p.grad for p in parameters if p.grad is not None]
    
    if len(grads) == 0:
        return torch.tensor(0.0)
    
    # Compute total norm squared
    total_norm_sq = sum(g.norm(2) ** 2 for g in grads)
    total_norm = torch.sqrt(total_norm_sq)
    
    # Clip if norm exceeds max_norm
    clip_coef = max_norm / (total_norm + 1e-6)
    if clip_coef < 1.0:
        for param in parameters:
            if param.grad is not None:
                param.grad.data.mul_(clip_coef)
    
    return total_norm


def token_accuracy(logits: Tensor, targets: Tensor, ignore_index: int = -100) -> Tensor:
    """
    Compute token-level accuracy for language modeling.
    
    Computes the fraction of tokens where the predicted token (argmax of logits)
    matches the target token, ignoring positions where target equals ignore_index.
    
    Args:
        logits: Predicted logits of shape (N, C) where N is the number of tokens
                and C is the vocabulary size
        targets: Ground truth token indices of shape (N,)
        ignore_index: Target value to ignore when computing accuracy (default: -100)
    
    Returns:
        Scalar tensor containing the accuracy (between 0 and 1)
    
    Example:
        >>> logits = torch.tensor([[2.0, 1.0, 0.5], [0.1, 3.0, 0.2], [1.0, 0.5, 2.5]])
        >>> targets = torch.tensor([0, 1, 2])
        >>> token_accuracy(logits, targets)
        tensor(1.)  # All predictions correct: argmax gives [0, 1, 2]
        
        >>> logits = torch.tensor([[2.0, 1.0], [0.1, 3.0], [1.0, 0.5]])
        >>> targets = torch.tensor([1, 1, 0])
        >>> token_accuracy(logits, targets)
        tensor(0.6667)  # 2 out of 3 correct
    """
    # TODO: Implement token accuracy
    # Get predictions (argmax of logits)
    pred = logits.argmax(dim=-1)
    
    # Create mask for valid (non-ignored) positions
    valid_mask = targets != ignore_index
    
    # Count correct predictions only for valid positions
    if valid_mask.sum() == 0:
        return torch.tensor(0.0)
    
    correct = (pred[valid_mask] == targets[valid_mask]).sum().float()
    total_valid = valid_mask.sum().float()
    
    return correct / total_valid


def perplexity(logits: Tensor, targets: Tensor, ignore_index: int = -100) -> Tensor:
    """
    Compute perplexity for language modeling.
    
    Perplexity is defined as exp(cross_entropy_loss). It measures how well the
    probability distribution predicted by the model matches the actual distribution
    of the tokens. Lower perplexity indicates better prediction.
    
    Args:
        logits: Predicted logits of shape (N, C) where N is the number of tokens
                and C is the vocabulary size
        targets: Ground truth token indices of shape (N,)
        ignore_index: Target value to ignore when computing perplexity (default: -100)
    
    Returns:
        Scalar tensor containing the perplexity (always >= 1)
    
    Example:
        >>> # Perfect predictions (one-hot logits matching targets)
        >>> logits = torch.tensor([[10.0, 0.0, 0.0], [0.0, 10.0, 0.0], [0.0, 0.0, 10.0]])
        >>> targets = torch.tensor([0, 1, 2])
        >>> perplexity(logits, targets)
        tensor(1.0001)  # Close to 1 (perfect)
        
        >>> # Uniform predictions (high uncertainty)
        >>> logits = torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])
        >>> targets = torch.tensor([0, 1, 2])
        >>> perplexity(logits, targets)
        tensor(3.)  # Equal to vocab_size (worst case for uniform)
    """
    # TODO: Implement perplexity
    # Perplexity = exp(cross_entropy_loss)
    # Need to compute cross-entropy only on non-ignored targets
    
    # Create mask for valid positions
    valid_mask = targets != ignore_index
    
    if valid_mask.sum() == 0:
        return torch.tensor(1.0)
    
    # Filter to only valid logits and targets
    valid_logits = logits[valid_mask]
    valid_targets = targets[valid_mask]
    
    # Compute cross-entropy on valid positions
    max_logits, _ = valid_logits.max(dim=-1, keepdim=True)
    shifted = valid_logits - max_logits
    exp_shifted = torch.exp(shifted)
    sum_exp = exp_shifted.sum(dim=-1, keepdim=True)
    log_softmax = shifted - torch.log(sum_exp)
    
    nll = -log_softmax.gather(1, valid_targets.unsqueeze(1)).squeeze(1)
    ce_loss = nll.mean()
    
    return torch.exp(ce_loss)
